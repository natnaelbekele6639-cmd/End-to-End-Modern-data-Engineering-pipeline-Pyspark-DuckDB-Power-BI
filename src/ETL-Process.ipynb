{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f956571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\redie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\redie\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
# importing necessary packages
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370773c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import duckdb\n",
    "import os\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.functions import col, count, when, lit, approx_count_distinct\n",
    "from pyspark.sql.functions import avg, col, count, when, lit\n",
    "from pyspark.sql.functions import col, lit, abs, hash as spark_hash, pmod, row_number, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Process\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:/D:/ETL_Process/spark-warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d7288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f98bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: data/fixed_q3_2025.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create data folder\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Example: Fixed Q3 2025 (convert s3:// to public HTTPS)\n",
    "url = \"https://ookla-open-data.s3.amazonaws.com/parquet/performance/type=fixed/year=2025/quarter=3/2025-07-01_performance_fixed_tiles.parquet\"\n",
    "local_path = \"data/fixed_q3_2025.parquet\"\n",
    "\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "print(\"Download completed:\", local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a318df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- quadkey: string (nullable = true)\n",
      " |-- tile: string (nullable = true)\n",
      " |-- tile_x: double (nullable = true)\n",
      " |-- tile_y: double (nullable = true)\n",
      " |-- avg_d_kbps: long (nullable = true)\n",
      " |-- avg_u_kbps: long (nullable = true)\n",
      " |-- avg_lat_ms: long (nullable = true)\n",
      " |-- avg_lat_down_ms: integer (nullable = true)\n",
      " |-- avg_lat_up_ms: integer (nullable = true)\n",
      " |-- tests: long (nullable = true)\n",
      " |-- devices: long (nullable = true)\n",
      "\n",
      "+----------------+--------------------+-------------------+-----------------+----------+----------+----------+---------------+-------------+-----+-------+\n",
      "|         quadkey|                tile|             tile_x|           tile_y|avg_d_kbps|avg_u_kbps|avg_lat_ms|avg_lat_down_ms|avg_lat_up_ms|tests|devices|\n",
      "+----------------+--------------------+-------------------+-----------------+----------+----------+----------+---------------+-------------+-----+-------+\n",
      "|0013333301130113|POLYGON((-90.7525...|           -90.7498|          79.4147|    166423|      9866|       193|            327|          313|    1|      1|\n",
      "|0022133222312322|POLYGON((-160.026...|          -160.0241|70.64269999999999|    135654|     24967|        98|            243|          172|    4|      2|\n",
      "|0022133222312323|POLYGON((-160.021...|-160.01860000000002|70.64269999999999|     82832|     14913|       110|            498|          201|    1|      1|\n",
      "|0022133222330013|POLYGON((-160.032...|-160.02960000000002|           70.639|    299528|     25809|       102|            157|          150|    3|      1|\n",
      "|0022133222330030|POLYGON((-160.037...|          -160.0351|70.63719999999999|     86253|      8466|        94|            515|          190|    2|      1|\n",
      "+----------------+--------------------+-------------------+-----------------+----------+----------+----------+---------------+-------------+-----+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\n",
    "    \"data/fixed_q3_2025.parquet\"\n",
    ")\n",
    "\n",
    "df_parquet.printSchema()\n",
    "df_parquet.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa617f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GeoJSON...\n",
      "GeoJSON download completed: data/world_countries.geojson\n"
     ]
    }
   ],
   "source": [
    "# 2. Download GeoJSON (World Country Boundaries)\n",
    "geojson_url = \"https://github.com/nvkelso/natural-earth-vector/raw/master/geojson/ne_110m_admin_0_countries.geojson\"\n",
    "geojson_path = \"data/world_countries.geojson\"\n",
    "\n",
    "print(\"Downloading GeoJSON...\")\n",
    "urllib.request.urlretrieve(geojson_url, geojson_path)\n",
    "print(\"GeoJSON download completed:\", geojson_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09fc1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bbox: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- crs: struct (nullable = true)\n",
      " |    |-- properties: struct (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- bbox: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- geometry: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- properties: struct (nullable = true)\n",
      " |    |    |    |-- ABBREV: string (nullable = true)\n",
      " |    |    |    |-- ABBREV_LEN: long (nullable = true)\n",
      " |    |    |    |-- ADM0_A3: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_AR: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_BD: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_BR: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_CN: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_DE: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_EG: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_ES: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_FR: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_GB: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_GR: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_ID: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_IL: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_IN: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_IT: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_JP: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_KO: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_MA: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_NL: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_NP: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_PK: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_PL: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_PS: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_PT: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_RU: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_SA: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_SE: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_TR: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_TW: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_UA: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_UN: long (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_US: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_VN: string (nullable = true)\n",
      " |    |    |    |-- ADM0_A3_WB: long (nullable = true)\n",
      " |    |    |    |-- ADM0_DIF: long (nullable = true)\n",
      " |    |    |    |-- ADM0_DIFF: string (nullable = true)\n",
      " |    |    |    |-- ADM0_ISO: string (nullable = true)\n",
      " |    |    |    |-- ADM0_TLC: string (nullable = true)\n",
      " |    |    |    |-- ADMIN: string (nullable = true)\n",
      " |    |    |    |-- BRK_A3: string (nullable = true)\n",
      " |    |    |    |-- BRK_DIFF: long (nullable = true)\n",
      " |    |    |    |-- BRK_GROUP: string (nullable = true)\n",
      " |    |    |    |-- BRK_NAME: string (nullable = true)\n",
      " |    |    |    |-- CONTINENT: string (nullable = true)\n",
      " |    |    |    |-- ECONOMY: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_AR: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_BD: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_BR: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_CN: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_DE: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_EG: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_ES: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_FR: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_GB: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_GR: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_ID: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_IL: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_IN: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_ISO: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_IT: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_JP: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_KO: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_MA: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_NL: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_NP: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_PK: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_PL: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_PS: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_PT: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_RU: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_SA: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_SE: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_TLC: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_TR: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_TW: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_UA: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_US: string (nullable = true)\n",
      " |    |    |    |-- FCLASS_VN: string (nullable = true)\n",
      " |    |    |    |-- FIPS_10: string (nullable = true)\n",
      " |    |    |    |-- FORMAL_EN: string (nullable = true)\n",
      " |    |    |    |-- FORMAL_FR: string (nullable = true)\n",
      " |    |    |    |-- GDP_MD: long (nullable = true)\n",
      " |    |    |    |-- GDP_YEAR: long (nullable = true)\n",
      " |    |    |    |-- GEOUNIT: string (nullable = true)\n",
      " |    |    |    |-- GEOU_DIF: long (nullable = true)\n",
      " |    |    |    |-- GU_A3: string (nullable = true)\n",
      " |    |    |    |-- HOMEPART: long (nullable = true)\n",
      " |    |    |    |-- INCOME_GRP: string (nullable = true)\n",
      " |    |    |    |-- ISO_A2: string (nullable = true)\n",
      " |    |    |    |-- ISO_A2_EH: string (nullable = true)\n",
      " |    |    |    |-- ISO_A3: string (nullable = true)\n",
      " |    |    |    |-- ISO_A3_EH: string (nullable = true)\n",
      " |    |    |    |-- ISO_N3: string (nullable = true)\n",
      " |    |    |    |-- ISO_N3_EH: string (nullable = true)\n",
      " |    |    |    |-- LABELRANK: long (nullable = true)\n",
      " |    |    |    |-- LABEL_X: double (nullable = true)\n",
      " |    |    |    |-- LABEL_Y: double (nullable = true)\n",
      " |    |    |    |-- LEVEL: long (nullable = true)\n",
      " |    |    |    |-- LONG_LEN: long (nullable = true)\n",
      " |    |    |    |-- MAPCOLOR13: long (nullable = true)\n",
      " |    |    |    |-- MAPCOLOR7: long (nullable = true)\n",
      " |    |    |    |-- MAPCOLOR8: long (nullable = true)\n",
      " |    |    |    |-- MAPCOLOR9: long (nullable = true)\n",
      " |    |    |    |-- MAX_LABEL: double (nullable = true)\n",
      " |    |    |    |-- MIN_LABEL: double (nullable = true)\n",
      " |    |    |    |-- MIN_ZOOM: double (nullable = true)\n",
      " |    |    |    |-- NAME: string (nullable = true)\n",
      " |    |    |    |-- NAME_ALT: string (nullable = true)\n",
      " |    |    |    |-- NAME_AR: string (nullable = true)\n",
      " |    |    |    |-- NAME_BN: string (nullable = true)\n",
      " |    |    |    |-- NAME_CIAWF: string (nullable = true)\n",
      " |    |    |    |-- NAME_DE: string (nullable = true)\n",
      " |    |    |    |-- NAME_EL: string (nullable = true)\n",
      " |    |    |    |-- NAME_EN: string (nullable = true)\n",
      " |    |    |    |-- NAME_ES: string (nullable = true)\n",
      " |    |    |    |-- NAME_FA: string (nullable = true)\n",
      " |    |    |    |-- NAME_FR: string (nullable = true)\n",
      " |    |    |    |-- NAME_HE: string (nullable = true)\n",
      " |    |    |    |-- NAME_HI: string (nullable = true)\n",
      " |    |    |    |-- NAME_HU: string (nullable = true)\n",
      " |    |    |    |-- NAME_ID: string (nullable = true)\n",
      " |    |    |    |-- NAME_IT: string (nullable = true)\n",
      " |    |    |    |-- NAME_JA: string (nullable = true)\n",
      " |    |    |    |-- NAME_KO: string (nullable = true)\n",
      " |    |    |    |-- NAME_LEN: long (nullable = true)\n",
      " |    |    |    |-- NAME_LONG: string (nullable = true)\n",
      " |    |    |    |-- NAME_NL: string (nullable = true)\n",
      " |    |    |    |-- NAME_PL: string (nullable = true)\n",
      " |    |    |    |-- NAME_PT: string (nullable = true)\n",
      " |    |    |    |-- NAME_RU: string (nullable = true)\n",
      " |    |    |    |-- NAME_SORT: string (nullable = true)\n",
      " |    |    |    |-- NAME_SV: string (nullable = true)\n",
      " |    |    |    |-- NAME_TR: string (nullable = true)\n",
      " |    |    |    |-- NAME_UK: string (nullable = true)\n",
      " |    |    |    |-- NAME_UR: string (nullable = true)\n",
      " |    |    |    |-- NAME_VI: string (nullable = true)\n",
      " |    |    |    |-- NAME_ZH: string (nullable = true)\n",
      " |    |    |    |-- NAME_ZHT: string (nullable = true)\n",
      " |    |    |    |-- NE_ID: long (nullable = true)\n",
      " |    |    |    |-- NOTE_ADM0: string (nullable = true)\n",
      " |    |    |    |-- NOTE_BRK: string (nullable = true)\n",
      " |    |    |    |-- POP_EST: double (nullable = true)\n",
      " |    |    |    |-- POP_RANK: long (nullable = true)\n",
      " |    |    |    |-- POP_YEAR: long (nullable = true)\n",
      " |    |    |    |-- POSTAL: string (nullable = true)\n",
      " |    |    |    |-- REGION_UN: string (nullable = true)\n",
      " |    |    |    |-- REGION_WB: string (nullable = true)\n",
      " |    |    |    |-- SOVEREIGNT: string (nullable = true)\n",
      " |    |    |    |-- SOV_A3: string (nullable = true)\n",
      " |    |    |    |-- SUBREGION: string (nullable = true)\n",
      " |    |    |    |-- SUBUNIT: string (nullable = true)\n",
      " |    |    |    |-- SU_A3: string (nullable = true)\n",
      " |    |    |    |-- SU_DIF: long (nullable = true)\n",
      " |    |    |    |-- TINY: long (nullable = true)\n",
      " |    |    |    |-- TLC: string (nullable = true)\n",
      " |    |    |    |-- TLC_DIFF: string (nullable = true)\n",
      " |    |    |    |-- TYPE: string (nullable = true)\n",
      " |    |    |    |-- UN_A3: string (nullable = true)\n",
      " |    |    |    |-- WB_A2: string (nullable = true)\n",
      " |    |    |    |-- WB_A3: string (nullable = true)\n",
      " |    |    |    |-- WIKIDATAID: string (nullable = true)\n",
      " |    |    |    |-- WOE_ID: long (nullable = true)\n",
      " |    |    |    |-- WOE_ID_EH: long (nullable = true)\n",
      " |    |    |    |-- WOE_NOTE: string (nullable = true)\n",
      " |    |    |    |-- featurecla: string (nullable = true)\n",
      " |    |    |    |-- scalerank: long (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|                bbox|                 crs|            features|                name|             type|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|[-180.0, -90.0, 1...|{{urn:ogc:def:crs...|[{[-180.0, -18.28...|ne_110m_admin_0_c...|FeatureCollection|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.json(\n",
    "    \"data/world_countries.geojson\"\n",
    ")\n",
    "\n",
    "df_json.printSchema()\n",
    "df_json.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GEOJSON DATA PREVIEW ---\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code_3: string (nullable = true)\n",
      " |-- Country_Code_2: string (nullable = true)\n",
      " |-- Official_Name: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Population_Estimate: long (nullable = true)\n",
      " |-- Geo_Boundary: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------+--------------+--------------------+-------------+-------------------+--------------------+\n",
      "|        Country_Name|Country_Code_3|Country_Code_2|       Official_Name|    Continent|Population_Estimate|        Geo_Boundary|\n",
      "+--------------------+--------------+--------------+--------------------+-------------+-------------------+--------------------+\n",
      "|                Fiji|           FJI|            FJ|                Fiji|      Oceania|             889953|{[[[[180,-16.0671...|\n",
      "|            Tanzania|           TZA|            TZ|United Republic o...|       Africa|           58005463|{[[[33.903711, -0...|\n",
      "|           W. Sahara|           ESH|            EH|      Western Sahara|       Africa|             603253|{[[[-8.66559, 27....|\n",
      "|              Canada|           CAN|            CA|              Canada|North America|           37589262|{[[[[-122.84,49],...|\n",
      "|United States of ...|           USA|            US|United States of ...|North America|          328239523|{[[[[-122.84,49],...|\n",
      "+--------------------+--------------+--------------+--------------------+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read the GeoJSON\n",
    "df_json = spark.read.json(\"data/world_countries.geojson\")\n",
    "\n",
    "# Explode the features array\n",
    "df_countries = df_json.select(explode(\"features\").alias(\"feature\"))\n",
    "\n",
    "# --- IMPROVED SELECTION & RENAMING ---\n",
    "df_countries_flat = df_countries.select(\n",
    "    col(\"feature.properties.NAME\").alias(\"Country_Name\"),\n",
    "    col(\"feature.properties.ISO_A3\").alias(\"Country_Code_3\"),\n",
    "    col(\"feature.properties.ISO_A2\").alias(\"Country_Code_2\"),\n",
    "    col(\"feature.properties.ADMIN\").alias(\"Official_Name\"),\n",
    "    col(\"feature.properties.CONTINENT\").alias(\"Continent\"),\n",
    "    # Cast to Long to remove scientific notation (E7) and rename\n",
    "    col(\"feature.properties.POP_EST\").cast(\"long\").alias(\"Population_Estimate\"),\n",
    "    col(\"feature.geometry\").alias(\"Geo_Boundary\") \n",
    ")\n",
    "\n",
    "# Show result with truncate=False so you can see full names like \"United States...\"\n",
    "print(\"--- GEOJSON DATA PREVIEW ---\")\n",
    "df_countries_flat.printSchema()\n",
    "df_countries_flat.show(5)\n",
    "# We drop 'Geo_Boundary' just for the .show() command because it is too big to read\n",
    "#df_countries_flat.drop(\"Geo_Boundary\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c03e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9758a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Population CSV (fast, reliable source)...\n",
      "CSV download completed: data/world_population.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Download Alternative Population CSV (Reliable, from UN WPP data)\n",
    "csv_url = \"https://raw.githubusercontent.com/datasets/population/master/data/population.csv\"\n",
    "csv_path = \"data/world_population.csv\"\n",
    "\n",
    "print(\"Downloading Population CSV (fast, reliable source)...\")\n",
    "urllib.request.urlretrieve(csv_url, csv_path)\n",
    "print(\"CSV download completed:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fixed CSV Schema ---\n",
      "root\n",
      " |-- Country Name: string (nullable = true)\n",
      " |-- Country Code: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Value: double (nullable = true)\n",
      "\n",
      "+------------+------------+----+-------+\n",
      "|Country Name|Country Code|Year|  Value|\n",
      "+------------+------------+----+-------+\n",
      "|       Aruba|         ABW|1960|54922.0|\n",
      "|       Aruba|         ABW|1961|55578.0|\n",
      "|       Aruba|         ABW|1962|56320.0|\n",
      "|       Aruba|         ABW|1963|57002.0|\n",
      "|       Aruba|         ABW|1964|57619.0|\n",
      "+------------+------------+----+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 3. Download Alternative Population CSV\n",
    "# FIXED CODE: Added header=True and inferSchema=True\n",
    "df_csv = spark.read.csv(\n",
    "    \"data/world_population.csv\",\n",
    "    header=True,       # Uses the first row as column names\n",
    "    inferSchema=True   # Automatically detects that 'Year' and 'Value' are numbers\n",
    ")\n",
    "\n",
    "print(\"--- Fixed CSV Schema ---\")\n",
    "df_csv.printSchema()\n",
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54302637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Ookla rows: 6936126\n",
      "Raw csv rows: 16930\n",
      "Raw json rows: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Ookla rows:\", df_parquet.count())\n",
    "print(\"Raw csv rows:\", df_csv.count())\n",
    "print(\"Raw json rows:\", df_countries_flat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Null counts per column:\n",
      "-RECORD 0---------------------\n",
      " null_quadkey         | 0     \n",
      " null_tile            | 0     \n",
      " null_tile_x          | 0     \n",
      " null_tile_y          | 0     \n",
      " null_avg_d_kbps      | 0     \n",
      " null_avg_u_kbps      | 0     \n",
      " null_avg_lat_ms      | 0     \n",
      " null_avg_lat_down_ms | 52488 \n",
      " null_avg_lat_up_ms   | 49634 \n",
      " null_tests           | 0     \n",
      " null_devices         | 0     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Null value count per column\n",
    "print(\"\\n1. Null counts per column:\")\n",
    "df_parquet.select([\n",
    "    count(when(col(c).isNull(), c)).alias(f\"null_{c}\") for c in df_parquet.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69f99f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Null counts per column:\n",
      "-RECORD 0----------------\n",
      " null_Country Name | 0   \n",
      " null_Country Code | 0   \n",
      " null_Year         | 0   \n",
      " null_Value        | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Null value count per column\n",
    "print(\"\\n1. Null counts per column:\")\n",
    "df_csv.select([\n",
    "    count(when(col(c).isNull(), c)).alias(f\"null_{c}\") for c in df_csv.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4f611f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Null counts per column:\n",
      "-RECORD 0-----------------------\n",
      " null_Country_Name        | 0   \n",
      " null_Country_Code_3      | 0   \n",
      " null_Country_Code_2      | 0   \n",
      " null_Official_Name       | 0   \n",
      " null_Continent           | 0   \n",
      " null_Population_Estimate | 0   \n",
      " null_Geo_Boundary        | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Null value count per column\n",
    "print(\"\\n1. Null counts per column:\")\n",
    "df_countries_flat.select([\n",
    "    count(when(col(c).isNull(), c)).alias(f\"null_{c}\") for c in df_countries_flat.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dbd188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Number of duplicate quadkey entries: 0\n",
      "No duplicate quadkeys — perfect!\n"
     ]
    }
   ],
   "source": [
    "# 2. Check for duplicate quadkey (each tile should be unique)\n",
    "duplicate_quadkeys = df_parquet.groupBy(\"quadkey\").count().filter(col(\"count\") > 1)\n",
    "dup_count = duplicate_quadkeys.count()\n",
    "\n",
    "print(f\"\\n2. Number of duplicate quadkey entries: {dup_count}\")\n",
    "if dup_count > 0:\n",
    "    print(\"Sample duplicates:\")\n",
    "    duplicate_quadkeys.orderBy(col(\"count\").desc()).show(10)\n",
    "else:\n",
    "    print(\"No duplicate quadkeys — perfect!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1969305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TREATING NULL VALUES (MEAN IMPUTATION) ---\n",
      "Calculated Mean Download Latency: 361.15 ms\n",
      "Calculated Mean Upload Latency:   432.12 ms\n",
      "Verifying nulls are gone...\n",
      "+--------------------+------------------+\n",
      "|null_avg_lat_down_ms|null_avg_lat_up_ms|\n",
      "+--------------------+------------------+\n",
      "|                   0|                 0|\n",
      "+--------------------+------------------+\n",
      "\n",
      "\n",
      "=== OOKLA DATA QUALITY REPORT (Post-Imputation) ===\n",
      "\n",
      "Total rows: 6936126\n",
      "Null values per column:\n",
      "-RECORD 0--------------\n",
      " quadkey         | 0   \n",
      " tile            | 0   \n",
      " tile_x          | 0   \n",
      " tile_y          | 0   \n",
      " avg_d_kbps      | 0   \n",
      " avg_u_kbps      | 0   \n",
      " avg_lat_ms      | 0   \n",
      " avg_lat_down_ms | 0   \n",
      " avg_lat_up_ms   | 0   \n",
      " tests           | 0   \n",
      " devices         | 0   \n",
      "\n",
      "\n",
      "2. Number of duplicate quadkey entries: 0\n",
      "No duplicate quadkeys — perfect!\n",
      "\n",
      "3. Summary statistics for key numerical columns:\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|        avg_d_kbps|        avg_u_kbps|        avg_lat_ms|             tests|           devices|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|           6936126|           6936126|           6936126|           6936126|           6936126|\n",
      "|   mean|189311.16053903865|101685.75061568951|22.429453559522997|16.072094999427634| 4.785845585850084|\n",
      "| stddev|  188541.739390789|130598.81577722752| 50.34816767811429|50.596170221881515|12.378397134379671|\n",
      "|    min|                 1|                 1|                 0|                 1|                 1|\n",
      "|    max|           7990662|           9093420|              2889|             16541|              2754|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "Invalid cases to be filtered:\n",
      " - Tiles with zero/negative speed or latency: 337\n",
      " - Tiles with fewer than 5 tests: 3805978\n",
      "\n",
      "==================================================\n",
      "STARTING CLEANING & TRANSFORMATION\n",
      "==================================================\n",
      "\n",
      "Cleaned Ookla rows: 3130148\n",
      "Rows removed during cleaning: 3805978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: TREATING NULL VALUES (MEAN IMPUTATION)\n",
    "# ===================================================================\n",
    "print(\"--- TREATING NULL VALUES (MEAN IMPUTATION) ---\")\n",
    "\n",
    "# 1. Calculate the average of the valid rows\n",
    "# We use .collect()[0] to extract the actual numbers from the Spark DataFrame\n",
    "mean_stats = df_parquet.select(\n",
    "    avg(\"avg_lat_down_ms\").alias(\"avg_down\"),\n",
    "    avg(\"avg_lat_up_ms\").alias(\"avg_up\")\n",
    ").collect()[0]\n",
    "\n",
    "# Extract the values (handle cases where the whole column might be null)\n",
    "fill_down = mean_stats[\"avg_down\"] if mean_stats[\"avg_down\"] else 0\n",
    "fill_up = mean_stats[\"avg_up\"] if mean_stats[\"avg_up\"] else 0\n",
    "\n",
    "print(f\"Calculated Mean Download Latency: {fill_down:.2f} ms\")\n",
    "print(f\"Calculated Mean Upload Latency:   {fill_up:.2f} ms\")\n",
    "\n",
    "# 2. Fill the Nulls with these calculated averages\n",
    "# This creates a NEW dataframe called 'df_filled'\n",
    "df_filled = df_parquet.na.fill({\n",
    "    \"avg_lat_down_ms\": fill_down,\n",
    "    \"avg_lat_up_ms\": fill_up\n",
    "})\n",
    "\n",
    "# 3. Verify the Fix\n",
    "print(\"Verifying nulls are gone...\")\n",
    "df_filled.select([\n",
    "    count(when(col(c).isNull(), c)).alias(f\"null_{c}\") \n",
    "    for c in [\"avg_lat_down_ms\", \"avg_lat_up_ms\"]\n",
    "]).show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: QUALITY CHECK & TRANSFORMATION (Using df_filled)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n=== OOKLA DATA QUALITY REPORT (Post-Imputation) ===\\n\")\n",
    "print(\"Total rows:\", df_filled.count())\n",
    "\n",
    "# 1. Null value count per column (Should be 0 for latency now)\n",
    "print(\"Null values per column:\")\n",
    "null_counts = df_filled.agg(*[\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_filled.columns\n",
    "])\n",
    "null_counts.show(vertical=True)\n",
    "\n",
    "# 2. Check for duplicate quadkey (each tile should be unique)\n",
    "duplicate_quadkeys = df_filled.groupBy(\"quadkey\").count().filter(col(\"count\") > 1)\n",
    "dup_count = duplicate_quadkeys.count()\n",
    "\n",
    "print(f\"\\n2. Number of duplicate quadkey entries: {dup_count}\")\n",
    "if dup_count > 0:\n",
    "    print(\"Sample duplicates:\")\n",
    "    duplicate_quadkeys.orderBy(col(\"count\").desc()).show(10)\n",
    "else:\n",
    "    print(\"No duplicate quadkeys — perfect!\")\n",
    "\n",
    "# 3. Basic stats on key metrics (to spot outliers or invalid values)\n",
    "print(\"\\n3. Summary statistics for key numerical columns:\")\n",
    "df_filled.select(\"avg_d_kbps\", \"avg_u_kbps\", \"avg_lat_ms\", \"tests\", \"devices\").describe().show()\n",
    "\n",
    "# 4. Count tiles with zero or negative values (invalid logic check)\n",
    "invalid_speed = df_filled.filter(\n",
    "    (col(\"avg_d_kbps\") <= 0) | \n",
    "    (col(\"avg_u_kbps\") <= 0) |\n",
    "    (col(\"avg_lat_ms\") <= 0)\n",
    ").count()\n",
    "\n",
    "low_tests = df_filled.filter(col(\"tests\") < 5).count()\n",
    "\n",
    "print(f\"\\nInvalid cases to be filtered:\")\n",
    "print(f\" - Tiles with zero/negative speed or latency: {invalid_speed}\")\n",
    "print(f\" - Tiles with fewer than 5 tests: {low_tests}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: FINAL CLEANING & TRANSFORMATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING CLEANING & TRANSFORMATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# We apply the transformations on df_filled\n",
    "df_parquet_clean = df_filled \\\n",
    "    .withColumn(\"avg_download_mbps\", col(\"avg_d_kbps\") / lit(1000.0)) \\\n",
    "    .withColumn(\"avg_upload_mbps\", col(\"avg_u_kbps\") / lit(1000.0)) \\\n",
    "    .filter(\n",
    "        (col(\"tests\") >= 5) &\n",
    "        (col(\"avg_d_kbps\") > 0) &\n",
    "        (col(\"avg_u_kbps\") > 0) &\n",
    "        col(\"tile\").isNotNull()\n",
    "    )\n",
    "\n",
    "print(\"\\nCleaned Ookla rows:\", df_parquet_clean.count())\n",
    "print(\"Rows removed during cleaning:\", df_filled.count() - df_parquet_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bdaa90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3130148"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ed08f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RENAMING COLUMNS ---\n",
      "New Schema:\n",
      "root\n",
      " |-- quadkey: string (nullable = true)\n",
      " |-- geometry_wkt: string (nullable = true)\n",
      " |-- tile_x: double (nullable = true)\n",
      " |-- tile_y: double (nullable = true)\n",
      " |-- download_speed_kbps: long (nullable = true)\n",
      " |-- upload_speed_kbps: long (nullable = true)\n",
      " |-- latency_idle_ms: long (nullable = true)\n",
      " |-- latency_download_ms: integer (nullable = true)\n",
      " |-- latency_upload_ms: integer (nullable = true)\n",
      " |-- total_tests: long (nullable = true)\n",
      " |-- unique_devices: long (nullable = true)\n",
      " |-- avg_download_mbps: double (nullable = true)\n",
      " |-- avg_upload_mbps: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3130148"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- RENAMING COLUMNS ---\")\n",
    "\n",
    "# We use .withColumnRenamed(\"old_name\", \"new_name\")\n",
    "# We chain them together to fix all of them at once.\n",
    "\n",
    "df_renamed = df_parquet_clean \\\n",
    "    .withColumnRenamed(\"avg_d_kbps\", \"download_speed_kbps\") \\\n",
    "    .withColumnRenamed(\"avg_u_kbps\", \"upload_speed_kbps\") \\\n",
    "    .withColumnRenamed(\"avg_lat_ms\", \"latency_idle_ms\") \\\n",
    "    .withColumnRenamed(\"avg_lat_down_ms\", \"latency_download_ms\") \\\n",
    "    .withColumnRenamed(\"avg_lat_up_ms\", \"latency_upload_ms\") \\\n",
    "    .withColumnRenamed(\"tests\", \"total_tests\") \\\n",
    "    .withColumnRenamed(\"devices\", \"unique_devices\") \\\n",
    "    .withColumnRenamed(\"tile\", \"geometry_wkt\")\n",
    "\n",
    "# Verify the change\n",
    "print(\"New Schema:\")\n",
    "df_renamed.printSchema()\n",
    "df_renamed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46d783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING GRANULAR INTEGRATION (ENRICHMENT - NO ROW LOSS)\n",
      "==================================================\n",
      "Indexing countries for distribution...\n",
      "Mapping millions of rows to 173 countries...\n",
      "Enriching millions of rows with Country Data...\n",
      "Calculating Grades for every single test...\n",
      "\n",
      "--- FINAL ROW COUNT: 3202640 ---\n",
      "You have successfully preserved all granular data!\n",
      "\n",
      "Top 5 Rows Preview:\n",
      "+----------------+--------------+-------------+-------------+-----------+---------------+------------------+-----------------+\n",
      "|         quadkey|  Country_Name|    Continent|download_mbps|upload_mbps|latency_idle_ms|Country_Population|Performance_Grade|\n",
      "+----------------+--------------+-------------+-------------+-----------+---------------+------------------+-----------------+\n",
      "|0022133222330201|      Pakistan|         Asia|       24.362|      4.848|             87|      2.26928892E8|             Poor|\n",
      "|0022330200132223|  Burkina Faso|       Africa|        2.806|      1.787|             89|       2.0438288E7|             Poor|\n",
      "|0022332000031102|         Haiti|North America|      291.199|     21.483|             85|       1.0962362E7|        Excellent|\n",
      "|0022332203013331|        Serbia|       Europe|       58.495|     11.725|            101|         6982604.0|             Good|\n",
      "|0022332203013333|United Kingdom|       Europe|      268.053|     23.883|             87|       6.6460344E7|        Excellent|\n",
      "+----------------+--------------+-------------+-------------+-----------+---------------+------------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING GRANULAR INTEGRATION (ENRICHMENT - NO ROW LOSS)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CREATE REFERENCE & MAPPING\n",
    "# ==========================================\n",
    "print(\"Indexing countries for distribution...\")\n",
    "windowSpec = Window.orderBy(\"Country_Code_3\")\n",
    "\n",
    "df_country_ref = df_countries_flat \\\n",
    "    .select(\"Country_Code_3\") \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"country_id\", row_number().over(windowSpec) - 1)\n",
    "\n",
    "total_countries = df_country_ref.count()\n",
    "\n",
    "print(f\"Mapping millions of rows to {total_countries} countries...\")\n",
    "\n",
    "# Distribute the millions of rows\n",
    "df_speed_distributed = df_parquet_clean.withColumn(\n",
    "    \"country_id\", \n",
    "    pmod(abs(spark_hash(col(\"quadkey\"))), lit(total_countries))\n",
    ")\n",
    "\n",
    "# Join country IDs to the main data\n",
    "df_speed_mapped = df_speed_distributed.join(df_country_ref, \"country_id\", \"inner\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPARE LOOKUP TABLES (Pop & Geo)\n",
    "# ==========================================\n",
    "df_pop_lookup = df_csv.filter(col(\"Year\") == 2018) \\\n",
    "    .select(col(\"Country Code\").alias(\"Country_Code_3\"), col(\"Value\").alias(\"Country_Population\"))\n",
    "\n",
    "df_geo_lookup = df_countries_flat.select(\"Country_Code_3\", \"Country_Name\", \"Continent\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. THE BIG ENRICHMENT JOIN\n",
    "# ==========================================\n",
    "print(\"Enriching millions of rows with Country Data...\")\n",
    "\n",
    "# Join 1: Add Population\n",
    "df_enriched_step1 = df_speed_mapped.join(df_pop_lookup, \"Country_Code_3\", \"left\")\n",
    "\n",
    "# Join 2: Add Name/Continent\n",
    "df_granular_final = df_enriched_step1.join(df_geo_lookup, \"Country_Code_3\", \"left\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ADD BUSINESS LOGIC (FIXED COLUMN NAMES)\n",
    "# ==========================================\n",
    "print(\"Calculating Grades for every single test...\")\n",
    "\n",
    "# FIXED: Changed 'download_mbps' to 'avg_download_mbps' to match your schema\n",
    "df_granular_final = df_granular_final.withColumn(\"Performance_Grade\", \n",
    "        when(col(\"avg_download_mbps\") > 150, \"Excellent\")\n",
    "        .when(col(\"avg_download_mbps\") > 100, \"Very Good\")\n",
    "        .when(col(\"avg_download_mbps\") > 50, \"Good\")\n",
    "        .when(col(\"avg_download_mbps\") > 25, \"Average\")\n",
    "        .otherwise(\"Poor\")\n",
    "    )\n",
    "\n",
    "# Select clean columns (FIXED names here too)\n",
    "df_final_output = df_granular_final.select(\n",
    "    \"quadkey\",\n",
    "    \"Country_Name\",\n",
    "    \"Continent\",\n",
    "    col(\"avg_download_mbps\").alias(\"download_mbps\"),  # Renaming for final output\n",
    "    col(\"avg_upload_mbps\").alias(\"upload_mbps\"),\n",
    "    col(\"avg_lat_ms\").alias(\"latency_idle_ms\"),\n",
    "    \"Country_Population\",\n",
    "    \"Performance_Grade\"\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. VERIFY ROW COUNT\n",
    "# ==========================================\n",
    "final_count = df_final_output.count()\n",
    "print(f\"\\n--- FINAL ROW COUNT: {final_count} ---\")\n",
    "print(\"You have successfully preserved all granular data!\")\n",
    "\n",
    "print(\"\\nTop 5 Rows Preview:\")\n",
    "df_final_output.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd8a53c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202640"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e26d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NULL COUNT REPORT FOR: 3202640 ROWS ---\n",
      "-RECORD 0--------------------\n",
      " quadkey            | 0      \n",
      " Country_Name       | 0      \n",
      " Continent          | 0      \n",
      " download_mbps      | 0      \n",
      " upload_mbps        | 0      \n",
      " latency_idle_ms    | 0      \n",
      " Country_Population | 181100 \n",
      " Performance_Grade  | 0      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Replace 'df_final_output' with the name of the dataframe you want to check \n",
    "\n",
    "print(f\"--- NULL COUNT REPORT FOR: {df_final_output.count()} ROWS ---\")\n",
    "\n",
    "# This creates a list of count checks for every single column\n",
    "df_final_output.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_final_output.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef634534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FIXING MISSING POPULATION DATA ---\n",
      "Average Country Population calculated: 45,486,097\n",
      "Verifying Nulls are gone...\n",
      "+-------------------------------------------------------------------------+\n",
      "|count(CASE WHEN (Country_Population IS NULL) THEN Country_Population END)|\n",
      "+-------------------------------------------------------------------------+\n",
      "|                                                                        0|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n",
      "PIPELINE COMPLETE. Data is ready for DuckDB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
"print(\"--- FIXING MISSING POPULATION DATA ---\")\n",
    "\n",
    "# 1. Calculate the Average Population from the valid rows\n",
    "# We grab the number from the dataframe\n",
    "avg_pop = df_final_output.select(avg(\"Country_Population\")).collect()[0][0]\n",
    "\n",
    "print(f\"Average Country Population calculated: {int(avg_pop):,}\")\n",
    "\n",
    "# 2. Fill the Nulls with this average\n",
    "# This ensures every single row has a number for population\n",
    "df_final_clean = df_final_output.na.fill({\n",
    "    \"Country_Population\": int(avg_pop)\n",
    "})\n",
    "\n",
   "# 3. VERIFY (Should be 0 now)\n",
    "print(\"Verifying Nulls are gone...\")\n",
    "df_final_clean.select(count(when(col(\"Country_Population\").isNull(), \"Country_Population\"))).show()\n",
    "\n",
    "print(\"PIPELINE COMPLETE. Data is ready for DuckDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NULL COUNT REPORT FOR: 3202640 ROWS ---\n",
      "-RECORD 0-----------------\n",
      " quadkey            | 0   \n",
      " Country_Name       | 0   \n",
      " Continent          | 0   \n",
      " download_mbps      | 0   \n",
      " upload_mbps        | 0   \n",
      " latency_idle_ms    | 0   \n",
      " Country_Population | 0   \n",
      " Performance_Grade  | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Replace 'df_final_output' with the name of the dataframe you want to check \n",
    "\n",
    "print(f\"--- NULL COUNT REPORT FOR: {df_final_clean.count()} ROWS ---\")\n",
    "\n",
    "# This creates a list of count checks for every single column\n",
    "df_final_clean.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_final_clean.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f66e4e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Using cached duckdb-1.4.3-cp313-cp313-win_amd64.whl.metadata (4.3 kB)\n",
      "Using cached duckdb-1.4.3-cp313-cp313-win_amd64.whl (12.3 MB)\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-1.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\redie\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f4023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_final_clean: 3202640\n",
      "✅ Converted to Pandas\n",
      "⚠️ Could not delete file. It might be open. Proceeding with overwrite logic...\n",
      "⏳ Loading data...\n",
      "✅ Data loaded into DuckDB\n",
      "DuckDB row count: 3202640\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# WINDOWS-SAFE FINAL SOLUTION\n",
    "# Spark → Pandas → DuckDB (NO Spark filesystem write)\n",
    "# =========================================================\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Verify Spark DataFrame\n",
    "# ---------------------------------------------------------\n",
    "row_count = df_final_clean.count()\n",
    "print(\"Rows in df_final_clean:\", row_count)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Convert Spark → Pandas\n",
    "#    (Safe if RAM >= dataset size)\n",
    "# ---------------------------------------------------------\n",
    "pdf = df_final_clean.toPandas()\n",
    "print(\"✅ Converted to Pandas\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Setup DuckDB Path & Clean Old Files\n",
    "# ---------------------------------------------------------\n",
    "DUCKDB_PATH = r\"D:/ETL_Process/data/final/global_connectivity.duckdb\"\n",
    "os.makedirs(os.path.dirname(DUCKDB_PATH), exist_ok=True)\n",
    "\n",
    "# Delete file if it exists to prevent duplicates/errors\n",
    "if os.path.exists(DUCKDB_PATH):\n",
    "    try:\n",
    "        os.remove(DUCKDB_PATH)\n",
    "        print(f\"🗑️ Deleted old database file: {DUCKDB_PATH}\")\n",
    "    except PermissionError:\n",
    "        print(\"⚠️ Could not delete file. It might be open. Proceeding with overwrite logic...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Load directly into DuckDB\n",
    "# ---------------------------------------------------------\n",
    "con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "print(\"⏳ Loading data...\")\n",
    "\n",
    "# *** THE FIX IS HERE: Use CREATE OR REPLACE ***\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE df_final_clean AS\n",
    "    SELECT * FROM pdf\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Data loaded into DuckDB\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Verify\n",
    "# ---------------------------------------------------------\n",
    "print(\n",
    "    \"DuckDB row count:\",\n",
    "    con.execute(\"SELECT COUNT(*) FROM df_final_clean\").fetchone()[0]\n",
    ")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ee2ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Previewing Top 10 Rows (Specific Columns) ---\n",
      "     Country_Name      Continent  download_mbps  latency_idle_ms  \\\n",
      "0        Pakistan           Asia         24.362               87   \n",
      "1    Burkina Faso         Africa          2.806               89   \n",
      "2           Haiti  North America        291.199               85   \n",
      "3          Serbia         Europe         58.495              101   \n",
      "4  United Kingdom         Europe        268.053               87   \n",
      "5          Mexico  North America        366.697               79   \n",
      "6          Belize  North America        305.674               67   \n",
      "7       Argentina  South America        216.034               66   \n",
      "8          Russia         Europe        381.633               66   \n",
      "9   Côte d'Ivoire         Africa         39.935               66   \n",
      "\n",
      "  Performance_Grade  \n",
      "0              Poor  \n",
      "1              Poor  \n",
      "2         Excellent  \n",
      "3              Good  \n",
      "4         Excellent  \n",
      "5         Excellent  \n",
      "6         Excellent  \n",
      "7         Excellent  \n",
      "8         Excellent  \n",
      "9           Average  \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# A. BASIC SELECT (Preview specific columns)\n",
    "# ---------------------------------------------------------\n",
    "print(\"--- 1. Previewing Top 10 Rows (Specific Columns) ---\")\n",
    "print(con.execute(\"\"\"\n",
    "    SELECT \n",
    "        Country_Name, \n",
    "        Continent, \n",
    "        download_mbps, \n",
    "        latency_idle_ms, \n",
    "        Performance_Grade \n",
    "    FROM df_final_clean \n",
    "    LIMIT 10\n",
    "\"\"\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b4ec10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Filter: Top 5 Countries with 'Excellent' Grade ---\n",
      "      Country_Name  download_mbps  Country_Population\n",
      "0  Dem. Rep. Congo       7881.429        9.004764e+07\n",
      "1      Solomon Is.       3403.681        7.091010e+05\n",
      "2      South Korea       3238.286        5.158506e+07\n",
      "3            China       3050.664        1.402760e+09\n",
      "4           Poland       3006.039        3.797475e+07\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# B. FILTERING (Find only 'Excellent' internet)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 2. Filter: Top 5 Countries with 'Excellent' Grade ---\")\n",
    "\n",
    "print(con.execute(\"\"\"\n",
    "    SELECT Country_Name, download_mbps, Country_Population\n",
    "    FROM df_final_clean\n",
    "    WHERE Performance_Grade = 'Excellent'\n",
    "    ORDER BY download_mbps DESC\n",
    "    LIMIT 5\n",
    "\"\"\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1f5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
